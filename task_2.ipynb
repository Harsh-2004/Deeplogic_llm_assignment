{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f484d1-9e3a-4864-b1f7-da5fbe5089cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\harsh\\anaconda3\\lib\\site-packages (5.9.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psutil --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1ed733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ClassifyResponseClassificationsItem(id='0a866668-c396-4c64-8d97-447b2602123e', input='Kraxi GmbH Kraxi GmbH Flugzeugallee 17 12345 Papierfeld Deutschland Flugzeugallee 17 12345 Papierfeld Deutschland Tel 0123 4567 Fax 0123 4568 infokraxicom wwwkraxicom Kraxi GmbH Sitz der Gesellschaft UStIdNr Postbank MÃ¼nchen GF Paul Kraxi MÃ¼nchen HRB 999999 DE123456789 IBAN DE28700100809999999999 PapierfliegerVertriebsGmbH Helga Musterfrau Rabattstr 25 34567 Osterhausen Deutschland Rechnungsnummer 201903 Liefer und Rechnungsdatum 8 Mai 2019 Kundennummer 987654 Ihre Auftragsnummer ABC123 BetrÃ¤ge in EUR Pos Artikelbeschreibung Menge Preis Betrag 1 Superdrachen 2 2000 4000 2 Turbo Flyer 5 4000 20000 3 SturzflugGeier 1 18000 18000 4 Eisvogel 3 5000 15000 5 Storch 10 2000 20000 6 Adler 1 7500 7500 7 Kostenlose Zugabe 1 000 000 Rechnungssumme netto 84500 zuzÃ¼glich 19 MwSt 16055 Rechnungssumme brutto 100555 Zahlbar innerhalb von 30 Tagen netto auf unser Konto Bitte geben Sie dabei die Rechnungsnummer an SkontoabzÃ¼ge werden nicht akzeptiert\\n', prediction='Spam', predictions=['Spam'], confidence=0.5286405, confidences=[0.5286405], labels={'Not spam': ClassifyResponseClassificationsItemLabelsValue(confidence=0.4713595), 'Spam': ClassifyResponseClassificationsItemLabelsValue(confidence=0.5286405)}, classification_type='single-label')]\n",
      "The letter informs the customer, Helga Musterfrau of PapierfliegerVertriebsGmbH, about the order details and the invoice. The order has several different articles, with their description, quantity, price and amount. The total amount to pay is 84500 EUR, including 19% VAT, the total invoice sum is 100555 EUR. The payment should be done within 30 days to the account of Kraxi GmbH, with the specified IBAN and SWIFT code. The company's information is also provided at the beginning of the letter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_result)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(summarize_text(document_text))\n\u001b[1;32m---> 67\u001b[0m translate_document(document_text,target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m, in \u001b[0;36mtranslate_document\u001b[1;34m(text, target)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_document\u001b[39m(text,target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     open_llama3b_kor \u001b[38;5;241m=\u001b[39m LLMtranslator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenlm-research/open_llama_3b\u001b[39m\u001b[38;5;124m'\u001b[39m, target_lang\u001b[38;5;241m=\u001b[39mtarget, translator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     11\u001b[0m     trnaslated_answer \u001b[38;5;241m=\u001b[39m open_llama3b_kor\u001b[38;5;241m.\u001b[39mgenerate(text)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (trnaslated_answer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transllm\\core.py:48\u001b[0m, in \u001b[0;36mLLMtranslator.__init__\u001b[1;34m(self, model_path, target_lang, translator, torch_dtype, offload_folder, device_map, google_api_key, deepl_api_key, bard_api_key, openai_api_key, openai_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lang \u001b[38;5;241m=\u001b[39m target_lang\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     49\u001b[0m     model_path, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map, offload_folder\u001b[38;5;241m=\u001b[39moffload_folder\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3850\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3841\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3842\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3843\u001b[0m     (\n\u001b[0;32m   3844\u001b[0m         model,\n\u001b[0;32m   3845\u001b[0m         missing_keys,\n\u001b[0;32m   3846\u001b[0m         unexpected_keys,\n\u001b[0;32m   3847\u001b[0m         mismatched_keys,\n\u001b[0;32m   3848\u001b[0m         offload_index,\n\u001b[0;32m   3849\u001b[0m         error_msgs,\n\u001b[1;32m-> 3850\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   3851\u001b[0m         model,\n\u001b[0;32m   3852\u001b[0m         state_dict,\n\u001b[0;32m   3853\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[0;32m   3854\u001b[0m         resolved_archive_file,\n\u001b[0;32m   3855\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3856\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   3857\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   3858\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[0;32m   3859\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[0;32m   3860\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   3861\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   3862\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m   3863\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[0;32m   3864\u001b[0m         is_quantized\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES),\n\u001b[0;32m   3865\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[0;32m   3866\u001b[0m     )\n\u001b[0;32m   3868\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[0;32m   3869\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3979\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3977\u001b[0m is_safetensors \u001b[38;5;241m=\u001b[39m archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_safetensors:\n\u001b[1;32m-> 3979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3980\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3981\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for them. Alternatively, make sure you have `safetensors` installed if the model you are using\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3982\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m offers the weights in this format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3983\u001b[0m     )\n\u001b[0;32m   3984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3985\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(offload_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format."
     ]
    }
   ],
   "source": [
    "# pip install cohere\n",
    "import cohere\n",
    "import transllm\n",
    "from cohere import ClassifyExample  \n",
    "from transllm import LLMtranslator\n",
    "\n",
    "\n",
    "#simple function to detect and translate text \n",
    "def translate_document(text,target='en'):\n",
    "    open_llama3b_kor = LLMtranslator('openlm-research/open_llama_3b', target_lang=target, translator='google') \n",
    "    trnaslated_answer = open_llama3b_kor.generate(text)\n",
    "    return (trnaslated_answer)\n",
    "    \n",
    "                                                                                                                                                                                    \n",
    "co = cohere.Client('eGsG0TZYRCtVh5gdsn0Gxx4yHxUgkzJgrGpvrQg5')\n",
    "examples = [\n",
    "    ClassifyExample(text=\"Dermatologists don't like her!\", label=\"Spam\"),\n",
    "    ClassifyExample(text=\"'Hello, open to this?'\", label=\"Spam\"),\n",
    "    ClassifyExample(\n",
    "        text=\"I need help please wire me $1000 right now\", label=\"Spam\"),\n",
    "    ClassifyExample(text=\"Nice to know you ;)\", label=\"Spam\"),\n",
    "    ClassifyExample(text=\"Please help me?\", label=\"Spam\"),\n",
    "    ClassifyExample(text=\"Your parcel will be delivered today\",\n",
    "                    label=\"Not spam\"),\n",
    "    ClassifyExample(\n",
    "        text=\"Review changes to our Terms and Conditions\", label=\"Not spam\"\n",
    "    ),\n",
    "    ClassifyExample(text=\"Weekly sync notes\", label=\"Not spam\"),\n",
    "    ClassifyExample(text=\"'Re: Follow up from today's meeting'\",\n",
    "                    label=\"Not spam\"),\n",
    "    ClassifyExample(text=\"Pre-read for tomorrow\", label=\"Not spam\"),\n",
    "]\n",
    "# Example function to generate embeddings\n",
    "def get_embeddings(text):\n",
    "    response = co.embed(model='large', texts=[text])\n",
    "    return response.embeddings\n",
    "def extract_entities(text):\n",
    "    response = co.extract_entities(model='large', text=text)\n",
    "    return response.entities\n",
    "\n",
    "def extract_relationships(entities):\n",
    "    # Implement relationship extraction logic here\n",
    "    relationships = []\n",
    "    # Example: Identify relationships based on co-occurrence in sentences\n",
    "    return relationships\n",
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    response = co.summarize( text=text,)\n",
    "    return response.summary\n",
    "def classify_document(text):\n",
    "    response = co.classify(inputs=text, examples=examples,)\n",
    "    return response.classifications\n",
    "\n",
    "\n",
    "      \n",
    "with open('output_1.txt', 'r') as file:\n",
    "    document_text = file.read()\n",
    "# Classify the document\n",
    "\n",
    "text_1 = [document_text]\n",
    "# text_1.append(document_text )\n",
    "\n",
    "classification_result = classify_document(text_1)\n",
    "print(classification_result)\n",
    "print(summarize_text(document_text))\n",
    "translate_document(document_text,target='en')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d833dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee00f11-0494-4320-aec3-6623c7287f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
